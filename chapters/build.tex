% !TeX root = ../main.tex

\chapter{基于事件相机的火焰数据集构建}
在之前绪论的火焰数据集部分，我们已经提到目前领域内常用的数据集以可见光相机拍摄为主，缺乏效果
优秀的，公开的基于事件相机的火焰数据集，本章中我们将使用DAVIS346事件相机收集相关的火焰数据并
着手制作一个初步的火焰检测数据集，后面我们统一用FIRE指代称呼。

\section{火焰数据集}
火焰检测数据集和其他的目标检测相似，也分为图象与视频两个大类，分别用于静态与动态不同的检测算法
训练与测试。例如著名的Dyntex公开库\cite{peteri2008comprehensive}中，提供了数百个动态视频纹理系列，其中的每个序列时间大于十秒，一秒25帧，
均为720x576的图像，同时每个均备注了一些相关的信息，包括采集的时间，设备，环境条件等。这里我们值得提及的是，视频数据集作为训练动态算法的连续帧视频文件，其帧数可以有所不同，由几秒到几分钟不等，但是往往
需要包含有火和无火两类场景，不同的环境条件，例如光照，风况，也均会对其效果造成不小影响。

同时目前也出现了大量的火焰烟雾场景为主的数据集，期望能够对早期烟雾进行识别而更早确定火灾场景。这里再以MIVIA数据集\cite{foggia2015real}为例，它分火焰集与烟雾集，包含的
火灾集视频序列共31个，分别是17个无火焰视频和14个火灾场景，均为不同环境拍摄，分类较为细致。同时它的烟雾集设置了很多远镜头烟雾场景，内含天空、云、强自然光等干扰，很考验
待测算法的准确度。此外还有中国科学技术大学火灾科学国家实验室制作的烟雾数据集，已经过人工标注，其中包含上万张的烟雾与非烟雾图像，包括真实烟雾与合成烟雾，
可以有效胜任深度网络训练。

\section{FlaDE检测数据集}
为研究火焰在事件领域的特征分布和设计火焰检测算法，我们制作了一套基于事件相机的火焰检测数据集，称之为 \textbf{Fla}me \textbf{D}etection based on \textbf{E}vent dataset (FlaDE)。该数据集由型号为DAVIS346的事件相机进行拍摄制作，该相机以$346\times260$的空间分辨率和\SI{1}{\micro\second}的时间分辨率输出事件数据（动态范围\SI{120}{\decibel}），输出带宽可达12MEPS。此外，该相机还能以最大40fps的帧率输出时间同步的灰度图像（动态范围\SI{56.7}{\decibel}），并内置6轴惯性测量单元（采样率\SI{8}{\kilo\hertz}）记录设备的角速度和加速度。

数据集拍摄场景选取在一片具有植被覆盖的开阔场地中，拍摄开始前，一个木垛火将被点燃，随即打开事件相机进行数据录制。拍摄过程中，相机被固定在三脚架上进行记录，并在整个数据采集期间始终保持静止。考虑到算法在不同监控视角下的适用性，我们选取有明显区别的拍摄角度和距离对火焰进行捕捉，并剪切成不同的数据片段，其中一个数据片段如图所示

事件相机对光线变化的高灵敏度虽然提供了优越的捕捉能力，但也带来了挑战，尤其是在复杂光照条件下的噪声问题。为此，我们特别录制了一系列夜间场景，在这些场景中，事件数据将伴随有大量的噪声信号，从而干扰火焰检测算法的性能（如图）。同时我们也测试了一些传统相机可能遇到的特殊情况，例如物体遮挡和图像过曝，以此检验事件相机在火焰检测中相对于传统相机来说是否具有特殊优势（如图）。上述数据片段的设置对火焰检测算法的鲁棒性提出了更高的要求。

此外，我们特意录制了包含常见干扰物（行人、灯光闪烁）的场景，如图所示。通过引入非火焰动态场景构建数据集的负样本集，有助于我们评估算法在误报率方面的表现。

最终的数据集构成如表所示，其中包含x个包含单火焰的序列作为正样本、x个行人和x个灯光的序列作为负样本补充。

\section{数据集预处理}
火焰检测数据集需要真实标签数据作为参考结果来与检测算法所预测的标签数据进行对比，因此我们为FlaDE数据集制作了真实的数据标签。注意到由于事件数据是存在于时间-空间领域中的四元组，因此我们需要对其进行预处理，以方便标注工作的进行，具体流程如下：

步骤一：采用RED递归去噪算法\cite{ding2023}对事件数据的噪声进行滤除。RED算法通过设计空间高斯核函数和时间高斯核函数进行逐事件的核密度估计，从而有效的滤除噪声，其中去噪参数（空间方差$\sigma_s$和时间方差$\sigma_t$）由人手动进行调整以达到最优的去噪结果。处理过程中我们采用该算法的DV版本\footnote{https://docs.inivation.com/}进行逐序列的手动调节，以确保尽可能多地抑制噪声，保留有效事件，同时我们也提供了具有挑战性的非去噪版本。

步骤二：将事件投影为二进制事件图像。我们参考\citet{kogler2009bio}提出的方法，将每组序列沿时间维度等间隔地进行切分，取$\Delta{t} \equiv \SI{33}{\milli\second}$，在同一时间间隔$\Delta{t}$内的事件被按坐标投影到相同分辨率的二维平面上，并按照极性构成由\{-1, 0, +1\}组成的二进制事件图像。

步骤三：融合二进制事件图像和灰度图像。由于事件相机仅对运动物体产生响应，单纯的二进制事件图像可能会影响标定人员对运动物体类别的误判，因此需要引入灰度图像，以方便标定人员对场景进行确定。我们通过计算同时间间隔内事件的时间平均，寻找到最近捕获的灰度图像，将两者按一定的权重进行融合，获得最终的标定图像。

整个标定过程在CVAT\footnote{https://www.cvat.ai/}上进行，由专业人员逐帧手工标定完成，并且所有标签都会由另一位专业人员重新审核，最大限度地保证了数据集结果的可信度。

\section{本章小结}
本章我们主要介绍了我们本次工作所使用的数据集的主要内容，制作过程与方法，后面的章节我们就会在这个FIRE数据集基础上
进行提取特征工作以及检测算法构建工作。